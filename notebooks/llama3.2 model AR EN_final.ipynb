{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01888b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig\n",
    "from datasets import DatasetDict\n",
    "from peft import prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea8d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"miscovery/Math_CoT_Arabic_English_Reasoning\", split=\"train\")\n",
    "# The dataset has fields: en_question, ar_question, en_answer, ar_answer, category, etc. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "def make_prompt_and_target(example):\n",
    "    # We'll make two versions: one English, one Arabic\n",
    "    en_in = f\"Language: English\\nQuestion: {example['en_question']}\\nAnswer:\"\n",
    "    ar_in = f\"Language: Arabic\\nQuestion: {example['ar_question']}\\nAnswer:\"\n",
    "\n",
    "    en_out = example['en_answer']\n",
    "    ar_out = example['ar_answer']\n",
    "\n",
    "    return {\n",
    "        \"input_text\": [en_in, ar_in],\n",
    "        \"target_text\": [en_out, ar_out],\n",
    "    }\n",
    "\n",
    "paired = dataset.map(make_prompt_and_target, batched=False)\n",
    "\n",
    "# Flatten the lists (because each example became 2)\n",
    "def flatten_examples(ex):\n",
    "    return {\n",
    "        \"input_text\": ex[\"input_text\"],\n",
    "        \"target_text\": ex[\"target_text\"],\n",
    "    }\n",
    "\n",
    "# Actually convert to a â€œflatâ€ dataset\n",
    "flat_dataset = paired.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7048359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en_question': 'A baker makes 24 cupcakes. She sells half of them at $3 each, and then sells the remaining cupcakes at $2 each. How much money does she make in total?', 'ar_question': 'ÙŠØ®Ø¨Ø² Ø®Ø¨Ø§Ø² 24 Ù‚Ø·Ø¹Ø© ÙƒØ¹Ùƒ. ÙŠØ¨ÙŠØ¹ Ù†ØµÙÙ‡Ø§ Ø¨Ø³Ø¹Ø± 3 Ø¯ÙˆÙ„Ø§Ø±Ø§Øª Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©ØŒ Ø«Ù… ÙŠØ¨ÙŠØ¹ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¨Ø³Ø¹Ø± 2 Ø¯ÙˆÙ„Ø§Ø± Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©. ÙƒÙ… ÙŠÙƒØ³Ø¨ Ù…Ù† Ø§Ù„Ù…Ø§Ù„ ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ØŸ', 'en_answer': '#step 1: Half of 24 cupcakes is 24 / 2 = 12 cupcakes. #step 2: She sells 12 cupcakes at $3 each, so she makes 12 * 3 = $36. #step 3: She sells the remaining 12 cupcakes at $2 each, so she makes 12 * 2 = $24. #step 4: In total, she makes $36 + $24 = $60. #answer: $60', 'ar_answer': '#Ø§Ù„Ø®Ø·ÙˆØ© 1: Ù†ØµÙ 24 Ù‚Ø·Ø¹Ø© ÙƒØ¹Ùƒ Ù‡Ùˆ 24 / 2 = 12 Ù‚Ø·Ø¹Ø© ÙƒØ¹Ùƒ. #Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ¨ÙŠØ¹ 12 Ù‚Ø·Ø¹Ø© ÙƒØ¹Ùƒ Ø¨Ø³Ø¹Ø± 3 Ø¯ÙˆÙ„Ø§Ø±Ø§Øª Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©ØŒ Ù„Ø°Ø§ ØªÙƒØ³Ø¨ 12 * 3 = 36 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§. #Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ¨ÙŠØ¹ 12 Ù‚Ø·Ø¹Ø© Ø§Ù„ÙƒØ¹Ùƒ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¨Ø³Ø¹Ø± 2 Ø¯ÙˆÙ„Ø§Ø± Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©ØŒ Ù„Ø°Ø§ ØªÙƒØ³Ø¨ 12 * 2 = 24 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§. #Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø¥Ø¬Ù…Ø§Ù„Ø§Ù‹ØŒ ØªÙƒØ³Ø¨ 36 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§ + 24 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§ = 60 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§. #Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: 60 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§', 'category': 'Mathematics - Arithmetic', 'en_q_word': 30, 'ar_q_word': 27, 'en_a_word': 62, 'ar_a_word': 68, 'input_text': ['Language: English\\nQuestion: A baker makes 24 cupcakes. She sells half of them at $3 each, and then sells the remaining cupcakes at $2 each. How much money does she make in total?\\nAnswer:', 'Language: Arabic\\nQuestion: ÙŠØ®Ø¨Ø² Ø®Ø¨Ø§Ø² 24 Ù‚Ø·Ø¹Ø© ÙƒØ¹Ùƒ. ÙŠØ¨ÙŠØ¹ Ù†ØµÙÙ‡Ø§ Ø¨Ø³Ø¹Ø± 3 Ø¯ÙˆÙ„Ø§Ø±Ø§Øª Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©ØŒ Ø«Ù… ÙŠØ¨ÙŠØ¹ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¨Ø³Ø¹Ø± 2 Ø¯ÙˆÙ„Ø§Ø± Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©. ÙƒÙ… ÙŠÙƒØ³Ø¨ Ù…Ù† Ø§Ù„Ù…Ø§Ù„ ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ØŸ\\nAnswer:'], 'target_text': ['#step 1: Half of 24 cupcakes is 24 / 2 = 12 cupcakes. #step 2: She sells 12 cupcakes at $3 each, so she makes 12 * 3 = $36. #step 3: She sells the remaining 12 cupcakes at $2 each, so she makes 12 * 2 = $24. #step 4: In total, she makes $36 + $24 = $60. #answer: $60', '#Ø§Ù„Ø®Ø·ÙˆØ© 1: Ù†ØµÙ 24 Ù‚Ø·Ø¹Ø© ÙƒØ¹Ùƒ Ù‡Ùˆ 24 / 2 = 12 Ù‚Ø·Ø¹Ø© ÙƒØ¹Ùƒ. #Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ¨ÙŠØ¹ 12 Ù‚Ø·Ø¹Ø© ÙƒØ¹Ùƒ Ø¨Ø³Ø¹Ø± 3 Ø¯ÙˆÙ„Ø§Ø±Ø§Øª Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©ØŒ Ù„Ø°Ø§ ØªÙƒØ³Ø¨ 12 * 3 = 36 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§. #Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ¨ÙŠØ¹ 12 Ù‚Ø·Ø¹Ø© Ø§Ù„ÙƒØ¹Ùƒ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¨Ø³Ø¹Ø± 2 Ø¯ÙˆÙ„Ø§Ø± Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©ØŒ Ù„Ø°Ø§ ØªÙƒØ³Ø¨ 12 * 2 = 24 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§. #Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø¥Ø¬Ù…Ø§Ù„Ø§Ù‹ØŒ ØªÙƒØ³Ø¨ 36 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§ + 24 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§ = 60 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§. #Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: 60 Ø¯ÙˆÙ„Ø§Ø±Ù‹Ø§']}\n"
     ]
    }
   ],
   "source": [
    "print(flat_dataset[0])  # Check the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5edc4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# 4. Format into LLaMA Instruct format\n",
    "def format_example(question, answer):\n",
    "    \"\"\"\n",
    "    Reformats question/answer into LLaMA Instruct chat format.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"<|start_header|>user<|end_header|>\\n\"\n",
    "        f\"{question}\\n\"\n",
    "        \"<|start_header|>assistant<|end_header|>\\n\"\n",
    "        f\"{answer}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 5. Tokenization with prompt masking\n",
    "def tokenize_fn(examples):\n",
    "    formatted_texts = []\n",
    "    for inp, tgt in zip(examples[\"input_text\"], examples[\"target_text\"]):\n",
    "        formatted = format_example(inp, tgt)\n",
    "        formatted_texts.append(formatted)\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    # Create labels and mask prompt\n",
    "    labels = []\n",
    "    for inp, tgt in zip(examples[\"input_text\"], examples[\"target_text\"]):\n",
    "        full_text = format_example(inp, tgt)\n",
    "\n",
    "        # Tokenize to get full input ids\n",
    "        full_ids = tokenizer(\n",
    "            full_text, truncation=True, max_length=1024\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # Tokenize only prompt part (question only)\n",
    "        prompt_text = (\n",
    "            \"<|begin_of_text|>\"\n",
    "            \"<|start_header|>user<|end_header|>\\n\"\n",
    "            f\"{inp}\\n\"\n",
    "            \"<|start_header|>assistant<|end_header|>\\n\"\n",
    "        )\n",
    "        prompt_ids = tokenizer(prompt_text)[\"input_ids\"]\n",
    "\n",
    "        # Mask prompt tokens\n",
    "        lbl = full_ids.copy()\n",
    "        prompt_len = len(prompt_ids)\n",
    "        lbl[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "        # Pad labels to max_length\n",
    "        if len(lbl) < 1024:\n",
    "            lbl = lbl + [-100] * (1024 - len(lbl))\n",
    "        else:\n",
    "            lbl = lbl[:1024]\n",
    "\n",
    "        labels.append(lbl)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# 6. Apply tokenization & clean dataset columns\n",
    "tokenized = flat_dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "        \"input_text\", \"target_text\",\n",
    "        \"en_question\", \"ar_question\",\n",
    "        \"en_answer\", \"ar_answer\",\n",
    "        \"category\",\n",
    "        \"en_q_word\", \"ar_q_word\",\n",
    "        \"en_a_word\", \"ar_a_word\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c23331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 128000, 27, 91, 2527, 8932, 91, 29, 882, 27, 91, 408, 8932, 91, 397, 681, 14126, 25, 6498, 1734, 14924, 25, 362, 76928, 3727, 220, 1187, 88993, 13, 3005, 31878, 4376, 315, 1124, 520, 400, 18, 1855, 11, 323, 1243, 31878, 279, 9861, 88993, 520, 400, 17, 1855, 13, 2650, 1790, 3300, 1587, 1364, 1304, 304, 2860, 33720, 77, 16533, 17898, 364, 14126, 25, 35217, 1734, 14924, 25, 74374, 115231, 40797, 75415, 125516, 220, 1187, 105077, 101454, 88041, 24102, 32173, 13, 74374, 116215, 51343, 106641, 100338, 111151, 103679, 220, 18, 103744, 103363, 100728, 103380, 101454, 105155, 100528, 102539, 69885, 108126, 74374, 116215, 100864, 121380, 54579, 100936, 117741, 111151, 103679, 220, 17, 119303, 100728, 103380, 101454, 105155, 100528, 102539, 13, 109025, 110333, 101108, 64337, 54579, 32482, 78373, 54579, 103969, 101735, 59, 77, 16533, 25, 4532, 27, 91, 2527, 8932, 91, 29, 78191, 27, 91, 408, 8932, 91, 397, 681, 2, 9710, 220, 16, 25, 26924, 315, 220, 1187, 88993, 374, 220, 1187, 611, 220, 17, 284, 220, 717, 88993, 13, 674, 9710, 220, 17, 25, 3005, 31878, 220, 717, 88993, 520, 400, 18, 1855, 11, 779, 1364, 3727, 220, 717, 353, 220, 18, 284, 400, 1927, 13, 674, 9710, 220, 18, 25, 3005, 31878, 279, 9861, 220, 717, 88993, 520, 400, 17, 1855, 11, 779, 1364, 3727, 220, 717, 353, 220, 17, 284, 400, 1187, 13, 674, 9710, 220, 19, 25, 763, 2860, 11, 1364, 3727, 400, 1927, 489, 400, 1187, 284, 400, 1399, 13, 674, 9399, 25, 400, 1399, 518, 5999, 32482, 112339, 108394, 220, 16, 25, 51343, 106641, 220, 1187, 105077, 101454, 88041, 24102, 32173, 101842, 220, 1187, 611, 220, 17, 284, 220, 717, 105077, 101454, 88041, 24102, 32173, 13, 674, 32482, 112339, 108394, 220, 17, 25, 40534, 116215, 220, 717, 105077, 101454, 88041, 24102, 32173, 111151, 103679, 220, 18, 103744, 103363, 100728, 103380, 101454, 105155, 100528, 102539, 69885, 126615, 116103, 101108, 220, 717, 353, 220, 18, 284, 220, 1927, 119303, 101333, 13, 674, 32482, 112339, 108394, 220, 18, 25, 40534, 116215, 220, 717, 105077, 101454, 101100, 24102, 32173, 54579, 100936, 117741, 111151, 103679, 220, 17, 119303, 100728, 103380, 101454, 105155, 100528, 102539, 69885, 126615, 116103, 101108, 220, 717, 353, 220, 17, 284, 220, 1187, 119303, 101333, 13, 674, 32482, 112339, 108394, 220, 19, 25, 86253, 100860, 112877, 118724, 116103, 101108, 220, 1927, 119303, 101333, 489, 220, 1187, 119303, 101333, 284, 220, 1399, 119303, 101333, 13, 674, 125854, 34190, 101637, 25, 220, 1399, 119303, 101333, 663, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 681, 2, 9710, 220, 16, 25, 26924, 315, 220, 1187, 88993, 374, 220, 1187, 611, 220, 17, 284, 220, 717, 88993, 13, 674, 9710, 220, 17, 25, 3005, 31878, 220, 717, 88993, 520, 400, 18, 1855, 11, 779, 1364, 3727, 220, 717, 353, 220, 18, 284, 400, 1927, 13, 674, 9710, 220, 18, 25, 3005, 31878, 279, 9861, 220, 717, 88993, 520, 400, 17, 1855, 11, 779, 1364, 3727, 220, 717, 353, 220, 17, 284, 400, 1187, 13, 674, 9710, 220, 19, 25, 763, 2860, 11, 1364, 3727, 400, 1927, 489, 400, 1187, 284, 400, 1399, 13, 674, 9399, 25, 400, 1399, 518, 5999, 32482, 112339, 108394, 220, 16, 25, 51343, 106641, 220, 1187, 105077, 101454, 88041, 24102, 32173, 101842, 220, 1187, 611, 220, 17, 284, 220, 717, 105077, 101454, 88041, 24102, 32173, 13, 674, 32482, 112339, 108394, 220, 17, 25, 40534, 116215, 220, 717, 105077, 101454, 88041, 24102, 32173, 111151, 103679, 220, 18, 103744, 103363, 100728, 103380, 101454, 105155, 100528, 102539, 69885, 126615, 116103, 101108, 220, 717, 353, 220, 18, 284, 220, 1927, 119303, 101333, 13, 674, 32482, 112339, 108394, 220, 18, 25, 40534, 116215, 220, 717, 105077, 101454, 101100, 24102, 32173, 54579, 100936, 117741, 111151, 103679, 220, 17, 119303, 100728, 103380, 101454, 105155, 100528, 102539, 69885, 126615, 116103, 101108, 220, 717, 353, 220, 17, 284, 220, 1187, 119303, 101333, 13, 674, 32482, 112339, 108394, 220, 19, 25, 86253, 100860, 112877, 118724, 116103, 101108, 220, 1927, 119303, 101333, 489, 220, 1187, 119303, 101333, 284, 220, 1399, 119303, 101333, 13, 674, 125854, 34190, 101637, 25, 220, 1399, 119303, 101333, 663, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5807c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = tokenized.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# temp (20%) â†’ validation (10%) + test (10%)\n",
    "split2 = split1['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine into DatasetDict\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": split1['train'],       \n",
    "    \"validation\": split2['train'],  \n",
    "    \"test\": split2['test']           \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e33f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2267\n",
      "283\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "print(len(final_dataset['train']))       # Number of training examples\n",
    "print(len(final_dataset['validation']))  # Number of validation examples\n",
    "print(len(final_dataset['test']))        # Number of test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5abaff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c28b0601db495ba435e7d9b6719c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,175,040 || all params: 3,221,924,864 || trainable%: 0.2848\n"
     ]
    }
   ],
   "source": [
    "# 5. Load model with bitsandbytes for quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Best for QLoRA\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Disable cache BEFORE attaching LoRA\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "# 6. Set up LoRA via PEFT\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient checkpointing ONLY AFTER LoRA is attached\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Print trainable parameters to confirm it's working\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf00e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anacoda\\envs\\py330\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_mathcot_qlora\",\n",
    "    # Training\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",   \n",
    "    eval_steps=200,                \n",
    "    eval_on_start=True,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # WandB Logging \n",
    "    report_to=\"wandb\",             \n",
    "    run_name=\"llama_mathcot_qlora\",\n",
    "\n",
    "    # Saving \n",
    "    save_total_limit=3,\n",
    "\n",
    "    # Other \n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.03,\n",
    "    dataloader_num_workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09031a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mislam-hitham\u001b[0m (\u001b[33mislam-hitham-king-salman-international-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68113d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=final_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57572dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\islam hitham\\Downloads\\wandb\\run-20251119_034742-z5blffow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/islam-hitham-king-salman-international-university/huggingface/runs/z5blffow' target=\"_blank\">llama_mathcot_qlora</a></strong> to <a href='https://wandb.ai/islam-hitham-king-salman-international-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/islam-hitham-king-salman-international-university/huggingface' target=\"_blank\">https://wandb.ai/islam-hitham-king-salman-international-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/islam-hitham-king-salman-international-university/huggingface/runs/z5blffow' target=\"_blank\">https://wandb.ai/islam-hitham-king-salman-international-university/huggingface/runs/z5blffow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42044dadb31a4acaacbdd6d033b96dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anacoda\\envs\\py330\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b25f32e2074626bb7806a91dcb37e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8697879910469055, 'eval_runtime': 143.7257, 'eval_samples_per_second': 1.969, 'eval_steps_per_second': 1.969, 'epoch': 0}\n",
      "{'loss': 0.9605, 'grad_norm': 0.9188120365142822, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.0}\n",
      "{'loss': 0.5789, 'grad_norm': 0.36892950534820557, 'learning_rate': 0.0001995806382799115, 'epoch': 0.18}\n",
      "{'loss': 0.4176, 'grad_norm': 0.38518038392066956, 'learning_rate': 0.00019603681998124953, 'epoch': 0.35}\n",
      "{'loss': 0.3953, 'grad_norm': 0.49780428409576416, 'learning_rate': 0.00018900514150473838, 'epoch': 0.53}\n",
      "{'loss': 0.3803, 'grad_norm': 0.30692413449287415, 'learning_rate': 0.00017874097897951737, 'epoch': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf069cbdac14a229321cb6a34c801dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3527951240539551, 'eval_runtime': 141.4199, 'eval_samples_per_second': 2.001, 'eval_steps_per_second': 2.001, 'epoch': 0.71}\n",
      "{'loss': 0.3738, 'grad_norm': 0.3786533772945404, 'learning_rate': 0.0001656171057204222, 'epoch': 0.88}\n",
      "{'loss': 0.3535, 'grad_norm': 0.3632659614086151, 'learning_rate': 0.00015011015386633952, 'epoch': 1.06}\n",
      "{'loss': 0.3147, 'grad_norm': 0.4334380030632019, 'learning_rate': 0.00013278330407925134, 'epoch': 1.24}\n",
      "{'loss': 0.3091, 'grad_norm': 0.383983850479126, 'learning_rate': 0.00011426583197856858, 'epoch': 1.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dca04651e254088a4b19b4aed3fc9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3264523148536682, 'eval_runtime': 141.6558, 'eval_samples_per_second': 1.998, 'eval_steps_per_second': 1.998, 'epoch': 1.41}\n",
      "{'loss': 0.2913, 'grad_norm': 0.4269470274448395, 'learning_rate': 9.52302541422768e-05, 'epoch': 1.59}\n",
      "{'loss': 0.2993, 'grad_norm': 0.3625357151031494, 'learning_rate': 7.636790368522208e-05, 'epoch': 1.76}\n",
      "{'loss': 0.2717, 'grad_norm': 0.4301329553127289, 'learning_rate': 5.8363822459400766e-05, 'epoch': 1.94}\n",
      "{'loss': 0.2535, 'grad_norm': 0.40233519673347473, 'learning_rate': 4.187188174000262e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09cd9cceac648bda39c7c9aa93262e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3202255368232727, 'eval_runtime': 140.5353, 'eval_samples_per_second': 2.014, 'eval_steps_per_second': 2.014, 'epoch': 2.12}\n",
      "{'loss': 0.2252, 'grad_norm': 0.4545200765132904, 'learning_rate': 2.749103496282306e-05, 'epoch': 2.29}\n",
      "{'loss': 0.2375, 'grad_norm': 0.41093361377716064, 'learning_rate': 1.5743564964878867e-05, 'epoch': 2.47}\n",
      "{'loss': 0.2335, 'grad_norm': 0.4645814001560211, 'learning_rate': 7.056115743798308e-06, 'epoch': 2.65}\n",
      "{'loss': 0.2505, 'grad_norm': 0.46202537417411804, 'learning_rate': 1.7441976235509917e-06, 'epoch': 2.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319c475d152f4d67b7fb6c9a3d965191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31805771589279175, 'eval_runtime': 141.0233, 'eval_samples_per_second': 2.007, 'eval_steps_per_second': 2.007, 'epoch': 2.82}\n",
      "{'train_runtime': 10770.7902, 'train_samples_per_second': 0.631, 'train_steps_per_second': 0.079, 'train_loss': 0.32036555922635174, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=849, training_loss=0.32036555922635174, metrics={'train_runtime': 10770.7902, 'train_samples_per_second': 0.631, 'train_steps_per_second': 0.079, 'total_flos': 1.1800933882763674e+17, 'train_loss': 0.32036555922635174, 'epoch': 2.996029995588884})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ddb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter / LoRA weights\n",
    "model.save_pretrained(\"./llama_mathcot_qlora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6fbc234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7bd30a53ee429a8d45608e84f98df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Anacoda\\envs\\py330\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English\n",
      "Question: If 4 apples cost $2, how much do 10 apples cost?\n",
      "Answer: $1\n",
      "Explanation: 4 apples cost $2, so 1 apple costs $2/4 = $0.50. 10 apples cost $0.50 x 10 = $5. The answer is $5, not $1. I apologize for the mistake earlier.\n",
      "The best answer is $5.\n",
      "Language: Arabic\n",
      "Question: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªÙƒÙ„ÙØ© 4 ØªÙØ§Ø­Ø§Øª 2 Ø¯ÙˆÙ„Ø§Ø±ØŒ ÙÙ…Ø§ Ù‡ÙŠ ØªÙƒÙ„ÙØ© 10 ØªÙØ§Ø­Ø§ØªØŸ\n",
      "Answer: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªÙƒÙ„ÙØ© 4 ØªÙØ§Ø­Ø§Øª 2 Ø¯ÙˆÙ„Ø§Ø±ØŒ ÙÙ…Ø§ Ù‡ÙŠ ØªÙƒÙ„ÙØ© 10 ØªÙØ§Ø­Ø§ØªØŸ\n",
      "Ø§Ù„Ø¬ÙˆØ§Ø¨: 20\n",
      "Ø§Ù„Ø´Ø±Ø­: Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø©ØŒ Ø³ÙˆÙ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ø¯ Ø§Ù„ØªÙƒÙ„ÙØ© Ù…Ù† 4 Ø¥Ù„Ù‰ 10 ØªÙØ§Ø­Ø§ØªØŒ Ù…Ø¹ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø£Ù† 10 ØªÙØ§Ø­Ø§Øª Ù‡ÙŠ 10/4 = 2.5 Ø£ÙˆÙ‚ÙŠØ©ØŒ ÙˆØªÙƒÙ„ÙØ© 2.5 Ø£ÙˆÙ‚ÙŠØ© Ù‡ÙŠ 2.5 * 2 = 5 Ø¯ÙˆÙ„Ø§Ø±. \n",
      "Ø§Ù„Ø´Ø±Ø­: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªÙƒÙ„ÙØ© 4 ØªÙØ§Ø­Ø§Øª 2 Ø¯ÙˆÙ„Ø§Ø±ØŒ ÙÙ…Ø§ Ù‡ÙŠ ØªÙƒÙ„ÙØ© 10 ØªÙØ§Ø­Ø§ØªØŸ\n",
      "\n",
      "Ø§Ù„Ø¬ÙˆØ§Ø¨: 20\n",
      "Ø§Ù„Ø´Ø±Ø­: Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø©ØŒ Ø³ÙˆÙ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ø¯ Ø§Ù„ØªÙƒÙ„ÙØ© Ù…Ù† 4 Ø¥Ù„Ù‰ 10 ØªÙØ§Ø­Ø§ØªØŒ Ù…Ø¹ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø£Ù† 10 ØªÙØ§Ø­Ø§Øª Ù‡ÙŠ 10/4 = 2\n"
     ]
    }
   ],
   "source": [
    "# Inference (load adapter + model again)\n",
    "peft_config = PeftConfig.from_pretrained(\"./llama_mathcot_qlora_adapter\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_answer(question: str, lang: str):\n",
    "    prompt = f\"Language: {lang}\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=200)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# Example:\n",
    "print(generate_answer(\"If 4 apples cost $2, how much do 10 apples cost?\", \"English\"))\n",
    "print(generate_answer(\"Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªÙƒÙ„ÙØ© 4 ØªÙØ§Ø­Ø§Øª 2 Ø¯ÙˆÙ„Ø§Ø±ØŒ ÙÙ…Ø§ Ù‡ÙŠ ØªÙƒÙ„ÙØ© 10 ØªÙØ§Ø­Ø§ØªØŸ\", \"Arabic\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
